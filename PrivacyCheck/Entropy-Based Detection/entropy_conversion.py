# -*- coding: utf-8 -*-
"""Entropy conversion.ipynb

Automatically generated by Colaboratory.


# Helper Functions
"""

import json
import os
import random

import tabulate
import numpy as np
import scipy.stats as stats
import scipy.signal as signal
import matplotlib.pyplot as plt

# Sliding window class to handle data being passed in one value at a time 
# This is based on number of data points, not time
class SlidingWindow:
  def __init__(self, size):
    assert size > 0
    self.size = size
    self.data = np.array([])
  def push(self, val):
    if len(self.data) < self.size:
      self.data = np.append(self.data, [val])
    else:
      self.data = np.append(self.data[1:], [val])

def calculate_shannon_entropy(data, precision=2):
  #sort continuous data into discrete buckets so entropy doesn't grow unbounded
  truncated_data = (data.round(decimals=precision) * (10 ** precision)).astype(int)
  value,counts = np.unique(truncated_data, return_counts=True)
  return stats.entropy(counts, base=2)

def get_data(filename):
  with open(filename) as data_file:
    data = np.array(json.load(data_file)['series'][0]['raw'])
    return data

def get_correct_changepoints(filepath):
  with open("/content/drive/MyDrive/School Work/Junior Year/Research/Input files/correct_changepoints.json") as file:
    correct_changepoints = json.load(file)
    filename = os.path.basename(filepath)
    assert filename in correct_changepoints
    return correct_changepoints[filename]

#REQUIRES: data starts off as normal, first changepoint marks anomalous
def get_data_labels(changepoints, length):
  is_anomaly = False
  tagged_data = np.zeros(length, dtype=bool)
  for i in range(length):
    if i in changepoints:
      is_anomaly = not is_anomaly
    tagged_data[i] = is_anomaly
  return tagged_data
  
def partition_array(values, partitions):
  #assumes values and partitions have same length
  assert(len(partitions) == len(values))

  p1 = np.array([])
  p2 = np.array([])

  for i in range(len(values)):
    if partitions[i]:
      p1 = np.append(p1, np.nan)
      p2 = np.append(p2, values[i])
    else:
      p1 = np.append(p1, values[i])
      p2 = np.append(p2, np.nan)
  
  return p1, p2

def get_sliding_window_entropies(data, window_size):
  window = SlidingWindow(window_size)
  for i in range(window_size):
    window.push(np.nan)
  entropies = []
  for datum in data:
    window.push(datum)
    entropy = calculate_shannon_entropy(window.data)
    entropies.append(entropy)
  return entropies

def plot_colored_entropy(filename, window_size=100):
  data = get_data(filename)
  calculate_shannon_entropy(data)
  entropies = get_sliding_window_entropies(data, window_size)
  changepoints = get_correct_changepoints(filename)
  labeled_data = get_data_labels(changepoints, len(data))

  #partition data into anomaly and valid
  valid, anomaly = partition_array(entropies, labeled_data)

  plt.plot(valid, color="b")
  plt.plot(anomaly, color="r")
  plt.xlabel("Timestep")
  plt.ylabel("Shannon Entropy (bits)")
  plt.title(filename.split("/")[-1])

def plot_colored_gradient(filename, window_size=100):
  data = get_data(filename)
  calculate_shannon_entropy(data)
  entropies = get_sliding_window_entropies(data, window_size)
  gradients = np.gradient(entropies)
  changepoints = get_correct_changepoints(filename)
  labeled_data = get_data_labels(changepoints, len(data))

  #partition data into anomaly and valid
  valid, anomaly = partition_array(gradients, labeled_data)
  
  plt.figure(figsize=(19.2,4.8)) 
  plt.plot(valid, color="b")
  plt.plot(anomaly, color="r")
  plt.xlabel("Timestep")
  plt.ylabel("Shannon Entropy (bits)")
  plt.title(filename.split("/")[-1])

def analyze_predictions(predicted_anomalies, real_anomalies):
  correct_anomalies = 0
  correct_normal = 0
  normal_classified_as_anomaly = 0
  anomaly_classified_as_normal = 0
  n = len(predicted_anomalies)

  for i in range(n):
    if real_anomalies[i] and predicted_anomalies[i]:
      correct_anomalies += 1
    elif real_anomalies[i] and not predicted_anomalies[i]:
      anomaly_classified_as_normal += 1
    elif not real_anomalies[i] and not predicted_anomalies[i]:
      correct_normal += 1
    elif not real_anomalies[i] and predicted_anomalies[i]:
      normal_classified_as_anomaly += 1

  return correct_anomalies, correct_normal, normal_classified_as_anomaly, anomaly_classified_as_normal

def calculate_f1_of_predictions(predicted_anomalies, real_anomalies):
  correct_anomalies, correct_normal, normal_classified_as_anomaly, anomaly_classified_as_normal = analyze_predictions(predicted_anomalies, real_anomalies)
  precision = correct_anomalies / (correct_anomalies + normal_classified_as_anomaly)
  recall = correct_anomalies / (correct_anomalies + anomaly_classified_as_normal)

  return stats.hmean([precision, recall])

def plot_incorrect_predictions(entropy_data, predicted_anomalies, real_anomalies):
  assert len(predicted_anomalies) == len(real_anomalies)
  correct_predictions = []
  for i in range(len(predicted_anomalies)):
    correct_predictions.append(predicted_anomalies[i] == real_anomalies[i])

  #partition array based on correctness
  incorrect, correct = partition_array(entropy_data, correct_predictions)

  plt.plot(incorrect, color="r")
  plt.plot(correct, color="k")
  plt.xlabel("Timestep")
  plt.ylabel("Shannon Entropy (bits)")
  plt.title(filename.split("/")[-1])

def gradient_anomaly_detection(data, window_size):
  e = get_sliding_window_entropies(data, window_size)
  der = np.gradient(e)
  return calculate_f1_of_predictions([g < 0.0099 for g in der], labeled_data)

#order of input: [True Positive    True Negative    False Positive    False Negative]
def print_results(data):
  headers = ["Method", "Sensitivity", "Specificity"]
  processed_data = []
  for row in data:
    sensitivity = row[1] / (row[1] + row[4])
    specificity = row[2] / (row[2] + row[3])
    processed_data.append([row[0], sensitivity, specificity])
  print(tabulate.tabulate(processed_data, headers=headers))





"""# Entropy conversion"""

filename = "/content/drive/MyDrive/School Work/Junior Year/Research/Input files/driver_scores_random400.json"
window_size = 100

data = get_data(filename)
entropies = get_sliding_window_entropies(data, window_size)
print_results(data)
# -*- coding: utf-8 -*-
"""BOCD.ipynb

Automatically generated by Colaboratory.

# Imports
"""

import numpy
import scipy.stats

# Copyright (c) 2014 Johannes Kulick
# Copyright (c) 2017-2018 David Tolpin
#
# Code initally borrowed from:
#    https://github.com/hildensia/bayesian_changepoint_detection
# under the MIT license.

class BOCD(object):
    def __init__(self, hazard_function, observation_likelihood):
        """Initializes th detector with zero observations.
        """
        self.t0 = 0
        self.t = -1
        self.growth_probs = numpy.array([1.])
        self.hazard_function = hazard_function
        self.observation_likelihood = observation_likelihood

    def update(self, x):
        """Updates changepoint probabilities with a new data point.
        """
        self.t += 1

        t = self.t - self.t0

        # allocate enough space
        if len(self.growth_probs) == t + 1:
            self.growth_probs = numpy.resize(self.growth_probs, (t + 1) * 2)

        # Evaluate the predictive distribution for the new datum under each of
        # the parameters.  This is the standard thing from Bayesian inference.
        pred_probs = self.observation_likelihood.pdf(x)

        # Evaluate the hazard function for this interval
        H = self.hazard_function(numpy.array(range(t + 1)))

        # Evaluate the probability that there *was* a changepoint and we're
        # accumulating the mass back down at r = 0.
        cp_prob = numpy.sum(self.growth_probs[0:t + 1] * pred_probs * H)

        # Evaluate the growth probabilities - shift the probabilities down and to
        # the right, scaled by the hazard function and the predictive
        # probabilities.
        self.growth_probs[1:t + 2] = self.growth_probs[0:t + 1] * pred_probs * (1-H)
        # Put back changepoint probability
        self.growth_probs[0] = cp_prob

        # Renormalize the run length probabilities for improved numerical
        # stability.
        self.growth_probs[0:t + 2] = self.growth_probs[0:t + 2] / \
            numpy.sum(self.growth_probs[0:t + 2])

        # Update the parameter sets for each possible run length.
        self.observation_likelihood.update_theta(x)

    def prune(self, t0):
        """prunes memory before time t0. That is, pruning at t=0
        does not change the memory. One should prune at times
        which are likely to correspond to changepoints.
        """
        self.t0 = t0
        self.observation_likelihood.prune(self.t - t0 + 1)


def constant_hazard(lam, r):
    """Computes the "constant" hazard, that is corresponding
    to Poisson process.
    """
    return 1/lam * numpy.ones(r.shape)


class StudentT:
    """Student's t predictive posterior.
    """
    def __init__(self, alpha, beta, kappa, mu):
        self.alpha0 = self.alpha = numpy.array([alpha])
        self.beta0 = self.beta = numpy.array([beta])
        self.kappa0 = self.kappa = numpy.array([kappa])
        self.mu0 = self.mu = numpy.array([mu])

    def pdf(self, data):
        """PDF of the predictive posterior.
        """
        return scipy.stats.t.pdf(x=data,
                                 df=2*self.alpha,
                                 loc=self.mu,
                                 scale=numpy.sqrt(self.beta * (self.kappa+1) /
                                                  (self.alpha * self.kappa)))

    def update_theta(self, data):
        """Bayesian update.
        """
        muT0 = numpy.concatenate((self.mu0, (self.kappa * self.mu + data) /
                                            (self.kappa + 1)))
        kappaT0 = numpy.concatenate((self.kappa0, self.kappa + 1.))
        alphaT0 = numpy.concatenate((self.alpha0, self.alpha + 0.5))
        betaT0 = numpy.concatenate((self.beta0,
                                    self.beta +
                                    (self.kappa * (data - self.mu)**2) /
                                    (2. * (self.kappa + 1.))))

        self.mu = muT0
        self.kappa = kappaT0
        self.alpha = alphaT0
        self.beta = betaT0

    def prune(self, t):
        """Prunes memory before t.
        """
        self.mu = self.mu[:t + 1]
        self.kappa = self.kappa[:t + 1]
        self.alpha = self.alpha[:t + 1]
        self.beta = self.beta[:t + 1]

"""# Helper Functions

"""

import json
import os
import random
  
import numpy as np
import scipy.stats as stats
import scipy.signal as signal
import matplotlib.pyplot as plt

# Sliding window class to handle data being passed in one value at a time 
# This is based on number of data points, not time
class SlidingWindow:
  def __init__(self, size):
    assert size > 0
    self.size = size
    self.data = np.array([])
  def push(self, val):
    if len(self.data) < self.size:
      self.data = np.append(self.data, [val])
    else:
      self.data = np.append(self.data[1:], [val])

def calculate_shannon_entropy(data, precision=2):
  #sort continuous data into discrete buckets so entropy doesn't grow unbounded
  truncated_data = (data.round(decimals=precision) * (10 ** precision)).astype(int)
  value,counts = np.unique(truncated_data, return_counts=True)
  return stats.entropy(counts, base=2)

def get_data(filename):
  with open(filename) as data_file:
    data = np.array(json.load(data_file)['series'][0]['raw'])
    return data

def get_correct_changepoints(filepath):
  with open("/content/drive/MyDrive/School Work/Junior Year/Research/Input files/correct_changepoints.json") as file:
    correct_changepoints = json.load(file)
    filename = os.path.basename(filepath)
    assert filename in correct_changepoints
    return correct_changepoints[filename]

#REQUIRES: data starts off as normal, first changepoint marks anomalous
def get_data_labels(changepoints, length):
  is_anomaly = False
  tagged_data = np.zeros(length, dtype=bool)
  for i in range(length):
    if i in changepoints:
      is_anomaly = not is_anomaly
    tagged_data[i] = is_anomaly
  return tagged_data
  
def partition_array(values, partitions):
  #assumes values and partitions have same length
  assert(len(partitions) == len(values))

  p1 = np.array([])
  p2 = np.array([])

  for i in range(len(values)):
    if partitions[i]:
      p1 = np.append(p1, np.nan)
      p2 = np.append(p2, values[i])
    else:
      p1 = np.append(p1, values[i])
      p2 = np.append(p2, np.nan)
  
  return p1, p2

def get_sliding_window_entropies(data, window_size):
  window = SlidingWindow(window_size)
  for i in range(window_size):
    window.push(np.nan)
  entropies = []
  for datum in data:
    window.push(datum)
    entropy = calculate_shannon_entropy(window.data)
    entropies.append(entropy)
  return np.array(entropies)

def plot_colored_entropy(filename, window_size=100):
  data = get_data(filename)
  calculate_shannon_entropy(data)
  entropies = get_sliding_window_entropies(data, window_size)
  changepoints = get_correct_changepoints(filename)
  labeled_data = get_data_labels(changepoints, len(data))

  #partition data into anomaly and valid
  valid, anomaly = partition_array(entropies, labeled_data)

  plt.plot(valid, color="b")
  plt.plot(anomaly, color="r")
  plt.xlabel("Timestep")
  plt.ylabel("Shannon Entropy (bits)")
  plt.title(filename.split("/")[-1])

def plot_colored_gradient(filename, window_size=100):
  data = get_data(filename)
  calculate_shannon_entropy(data)
  entropies = get_sliding_window_entropies(data, window_size)
  gradients = np.gradient(entropies)
  changepoints = get_correct_changepoints(filename)
  labeled_data = get_data_labels(changepoints, len(data))

  #partition data into anomaly and valid
  valid, anomaly = partition_array(gradients, labeled_data)
  
  plt.figure(figsize=(19.2,4.8)) 
  plt.plot(valid, color="b")
  plt.plot(anomaly, color="r")
  plt.xlabel("Timestep")
  plt.ylabel("Shannon Entropy (bits)")
  plt.title(filename.split("/")[-1])

def calculate_f1_of_predictions(predicted_anomalies, real_anomalies):
  correct_anomalies = 0
  correct_normal = 0
  normal_classified_as_anomaly = 0
  anomaly_classified_as_normal = 0

  for i in range(len(predicted_anomalies)):
    if real_anomalies[i] and predicted_anomalies[i]:
      correct_anomalies += 1
    elif real_anomalies[i] and not predicted_anomalies[i]:
      anomaly_classified_as_normal += 1
    elif not real_anomalies[i] and not predicted_anomalies[i]:
      correct_normal += 1
    elif not real_anomalies[i] and predicted_anomalies[i]:
      normal_classified_as_anomaly += 1

  precision = correct_anomalies / (correct_anomalies + normal_classified_as_anomaly)
  recall = correct_anomalies / (correct_anomalies + anomaly_classified_as_normal)

  # print(f"Precision: {precision}")
  # print(f"Recall: {recall}")

  return stats.hmean([precision, recall])

def plot_incorrect_predictions(entropy_data, predicted_anomalies, real_anomalies):
  assert len(predicted_anomalies) == len(real_anomalies)
  correct_predictions = []
  for i in range(len(predicted_anomalies)):
    correct_predictions.append(predicted_anomalies[i] == real_anomalies[i])

  #partition array based on correctness
  incorrect, correct = partition_array(entropy_data, correct_predictions)

  plt.plot(incorrect, color="r")
  plt.plot(correct, color="k")
  plt.xlabel("Timestep")
  plt.ylabel("Shannon Entropy (bits)")
  plt.title(filename.split("/")[-1])

def gradient_anomaly_detection(data, window_size):
  e = get_sliding_window_entropies(data, window_size)
  der = np.gradient(e)
  return calculate_f1_of_predictions([g < 0.0099 for g in der], labeled_data)



"""# Run BOCD on entropy data"""

# Get data from input filename and calculate entropies
filename = "/content/drive/MyDrive/School Work/Junior Year/Research/Input files/driver_scores_random2.json"
window_size = 100
data = get_data(filename)
entropies = get_sliding_window_entropies(data, window_size)
plt.plot(entropies)

# Run BOCD on it
bc = bocd.BayesianOnlineChangePointDetection(bocd.ConstantHazard(1000), bocd.StudentT(mu=0, kappa=1, alpha=1, beta=1))
rt_mle = np.empty(entropies.shape)
for i, d in enumerate(entropies):
    bc.update(d)
    rt_mle[i] = bc.rt
#plot results
plt.plot(entropies, alpha=0.5, label="observation")
index_changes = np.where(np.diff(rt_mle)<0)[0]
plt.scatter(index_changes, entropies[index_changes], c='green', label="change point")



"""# Test BOCD

"""

# Generate test data
test_signal = np.concatenate(
    [np.random.normal(0.7, 0.05, 300), 
     np.random.normal(1.5, 0.05, 300),
     np.random.normal(0.6, 0.05, 300),
     np.random.normal(1.3, 0.05, 300)])
plt.plot(test_signal)

DELAY = 15
LAMBDA = 10
ALPHA = 0.1
BETA = 1.
KAPPA = 1.
MU = 0.
DELAY = 15
THRESHOLD = 0.5
from functools import partial
bocd = BOCD(partial(constant_hazard, LAMBDA), StudentT(ALPHA, BETA, KAPPA, MU))
changepoints = []
for x in entropies[:DELAY]:
    bocd.update(x)
for x in entropies[DELAY:]:
    bocd.update(x)
    if bocd.growth_probs[DELAY] >= THRESHOLD:
        changepoints.append(bocd.t - DELAY + 1)

# Plot data with estimated change points
plt.plot(entropies, alpha=0.5, label="observation")
plt.scatter(changepoints, entropies[changepoints], c='green', label="change point")
print(changepoints)

